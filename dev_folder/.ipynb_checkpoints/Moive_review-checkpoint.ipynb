{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b52a195a",
   "metadata": {},
   "source": [
    "### Hypothesis \n",
    "\n",
    "1. Is there a significant difference in the average word count between positive and negative reviews?\n",
    "\n",
    "2. Is there a significant association between sentiment (positive) and the presence of specific words like \"amazing,\" \"great,\" and \"interesting\" in movie reviews?\n",
    "\n",
    "3. Is there a significant association between sentiment (negative) and the presence of specific words like \"terrible,\" \"bad,\" and \"boring\" in movie reviews?\n",
    "\n",
    "\n",
    "### Questions \n",
    "\n",
    "1. What is the distribution of sentiment labels in the movie reviews dataset, and how are they distributed among different sentiment categories?\n",
    "\n",
    "2. What does the distribution of word counts in movie reviews reveal about the length of reviews in the dataset?\n",
    "\n",
    "3. How does the word count of movie reviews relate to their sentiment? Is there a noticeable difference in word count between positive and negative reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1356296",
   "metadata": {},
   "source": [
    "## Installation of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec14aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !pip install gradio\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c332671",
   "metadata": {},
   "source": [
    "## Importation of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import string\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.base import TransformerMixin \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6defc181",
   "metadata": {},
   "source": [
    "## Data Collection & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5940adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Israel-Anaba/Movie_Review_Analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb76a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd Movie_Review_Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607bd18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('C:Assets\\Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb39008",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c566ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb88722",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb5e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d3923",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Columns Names: {list(data.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc9701",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=pd.read_csv('C:Assets\\Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799fa249",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b8792",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'content': 'review'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('review_file', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4703645",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d29b2f",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ede5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of sentiment labels\n",
    "sns.countplot(x='sentiment', data=data)\n",
    "plt.title('Distribution of Sentiment Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a134b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of each sentiment label\n",
    "sentiment_count = data['sentiment'].value_counts()\n",
    "\n",
    "print(sentiment_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f9d0e",
   "metadata": {},
   "source": [
    "#### Observation - The distribution of both classes (Positive & Negative) are evenly matched. This confirms that the class is balance and no need for a balancing the data to mitigate a class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = data['review'].str.findall(r'#\\w+')\n",
    "hashtags = [item for sublist in hashtags for item in sublist]\n",
    "hashtag_freq = Counter(hashtags)\n",
    "\n",
    "# Get the top 10 hashtags\n",
    "top_10_hashtags = dict(hashtag_freq.most_common(10))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_10_hashtags.keys(), top_10_hashtags.values())\n",
    "plt.xlabel('Hashtags')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 Hashtags')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eea206",
   "metadata": {},
   "source": [
    "#### Observation - The hashtags featured in the dataset is numeric. They could likely be consumers numbering their reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f93401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud for positive reviews\n",
    "positive_reviews = data[data['sentiment'] == 'positive']['review']\n",
    "positive_text = ' '.join(positive_reviews)\n",
    "positive_wordcloud = WordCloud(width=800, height=400).generate(positive_text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(positive_wordcloud, interpolation='bilinear')\n",
    "plt.title('Word Cloud for Positive Reviews')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f845d",
   "metadata": {},
   "source": [
    "#### Observation - This is a visual of the some of the most prevalent words in the Positive class. Words like : movie, watch director featured heavily. These terms are closely related to the domain of movies in relation to the Positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88796978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud for negative reviews\n",
    "negative_reviews = data[data['sentiment'] == 'negative']['review']\n",
    "negative_text = ' '.join(negative_reviews)\n",
    "negative_wordcloud = WordCloud(width=800, height=400).generate(negative_text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(negative_wordcloud, interpolation='bilinear')\n",
    "plt.title('Word Cloud for Negative Reviews')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333ac10d",
   "metadata": {},
   "source": [
    "#### Observation - This is a visual of the some of the most prevalent words in the Negative class. Words like : movie, bad and actor featured heavily. These terms are closely related to the domain of movies in relation to the Negaitive class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c12965",
   "metadata": {},
   "source": [
    "## HYPOTHESIS TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349ff0ce",
   "metadata": {},
   "source": [
    "### Question: Is a significant difference in the average word count between positive and negative reviews\n",
    "\n",
    "### H(0) - There is no significant difference in the average word count between positive and negative reviews \n",
    "\n",
    "### H(1)- There is a significant difference in the average word count between positive and negative reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ce0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate positive and negative reviews\n",
    "positive_reviews = data[data['sentiment'] == 'positive']\n",
    "negative_reviews = data[data['sentiment'] == 'negative']\n",
    "\n",
    "# Calculate word count for each review\n",
    "positive_reviews['word_count'] = positive_reviews['review'].apply(lambda x: len(x.split()))\n",
    "negative_reviews['word_count'] = negative_reviews['review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Perform a two-sample t-test\n",
    "t_stat, p_value = ttest_ind(positive_reviews['word_count'], negative_reviews['word_count'])\n",
    "\n",
    "# Define the significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Check the p-value\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference in word count between positive and negative reviews.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference in word count between positive and negative reviews.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f7379",
   "metadata": {},
   "source": [
    "#### Observation - The Hypothesis test infers that the average number of words counts in the classification of the review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f8b05d",
   "metadata": {},
   "source": [
    "### Question: Is there a significant association between sentiment (positive) and the presence of specific words like \"amazing,\" \"great,\" and \"interesting\" in movie reviews?\n",
    "\n",
    "### Hypotheses:\n",
    "\n",
    "### H(0): There is no significant association between sentiment and the presence of these specific words in movie reviews.\n",
    "### H(1): There is a significant association between sentiment and the presence of these specific words in movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8d9d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of words to test\n",
    "words_to_test = [\"amazing\", \"great\", \"interesting\"]\n",
    "\n",
    "for word in words_to_test:\n",
    "    # Calculate the count of positive reviews containing the word\n",
    "    count_positive_word = len(data[(data['sentiment'] == 'positive') & data['review'].str.contains(word)])\n",
    "\n",
    "    # Calculate the count of positive reviews without the word\n",
    "    count_positive_without_word = len(data[(data['sentiment'] == 'positive') & ~data['review'].str.contains(word)])\n",
    "\n",
    "    # Perform a chi-squared test for each word\n",
    "    contingency_table = [[count_positive_word, count_positive_without_word],\n",
    "                         [data['sentiment'].value_counts()['positive'] - count_positive_word, data['sentiment'].value_counts()['positive'] - count_positive_without_word]]\n",
    "\n",
    "    chi2, p, _, _ = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "    # Print the results for each word\n",
    "    print(f\"Word: '{word}'\")\n",
    "    print(f\"Count of positive reviews containing '{word}':\", count_positive_word)\n",
    "    print(f\"Count of positive reviews without '{word}':\", count_positive_without_word)\n",
    "    print(\"Chi-squared statistic:\", chi2)\n",
    "    print(\"p-value:\", p)\n",
    "\n",
    "    # Perform a hypothesis test for each word\n",
    "    alpha = 0.05\n",
    "    if p < alpha:\n",
    "        print(f\"Reject the null hypothesis: There's a significant association between sentiment and '{word}'.\")\n",
    "    else:\n",
    "        print(f\"Fail to reject the null hypothesis: No significant association between sentiment and '{word}'.\")\n",
    "    \n",
    "    # Add a separator for readability\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a744386",
   "metadata": {},
   "source": [
    "#### Observation - This Hypothesis Test infers that the prevalence of the words in a review text could translate in a Positive Sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42bcfb2",
   "metadata": {},
   "source": [
    "### Question: Is there a significant association between sentiment (negative) and the presence of specific words like \"terrible,\" \"boring,\" and \"bad\" in movie reviews?\n",
    "\n",
    "### Hypotheses:\n",
    "\n",
    "### H(0): There is no significant association between sentiment and the presence of these specific words in movie reviews.\n",
    "### H(1): There is a significant association between sentiment and the presence of these specific words in movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca24291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of words to test\n",
    "words_to_test = [\"terrible\", \"bad\", \"boring\"]\n",
    "\n",
    "for word in words_to_test:\n",
    "    # Calculate the count of negative reviews containing the word\n",
    "    count_negative_with_word = len(data[(data['sentiment'] == 'negative') & data['review'].str.contains(word)])\n",
    "\n",
    "    # Calculate the count of negative reviews without the word\n",
    "    count_negative_without_word = len(data[(data['sentiment'] == 'negative') & ~data['review'].str.contains(word)])\n",
    "\n",
    "    # Perform a chi-squared test for each word\n",
    "    contingency_table = [[count_negative_with_word, count_negative_without_word],\n",
    "                         [data['sentiment'].value_counts()['negative'] - count_negative_with_word, data['sentiment'].value_counts()['negative'] - count_negative_without_word]]\n",
    "\n",
    "    chi2, p, _, _ = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "    # Print the results for each word\n",
    "    print(f\"Word: '{word}'\")\n",
    "    print(\"Count of negative reviews containing the word:\", count_negative_with_word)\n",
    "    print(\"Count of negative reviews without the word:\", count_negative_without_word)\n",
    "    print(\"Chi-squared statistic:\", chi2)\n",
    "    print(\"p-value:\", p)\n",
    "\n",
    "    # Perform a hypothesis test for each word\n",
    "    alpha = 0.05\n",
    "    if p < alpha:\n",
    "        print(f\"Reject the null hypothesis: There's a significant association between sentiment and '{word}'.\")\n",
    "    else:\n",
    "        print(f\"Fail to reject the null hypothesis: No significant association between sentiment and '{word}'.\")\n",
    "    # Add a separator for readability\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6318633",
   "metadata": {},
   "source": [
    "#### Observation - This Hypothesis Test infers that the prevalence of the words in a review text could translate in a Negative Sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4bc7ef",
   "metadata": {},
   "source": [
    "  ## ANSWERING QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffed45",
   "metadata": {},
   "source": [
    "1. What is the distribution of sentiment labels in the movie reviews dataset, and how are they distributed among different sentiment categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c173ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of sentiment labels\n",
    "sentiment_counts = data['sentiment'].value_counts()\n",
    "plt.bar(sentiment_counts.index, sentiment_counts.values)\n",
    "plt.title('Distribution of Sentiment Labels')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85dc739",
   "metadata": {},
   "source": [
    "### Observation - The counts of the Sentiment class are leveled at about 12500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7b773",
   "metadata": {},
   "source": [
    "2. What does the distribution of word counts in movie reviews reveal about the length of reviews in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20fcb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word count for each review\n",
    "data['word_count'] = data['review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Plot the word count distribution\n",
    "plt.hist(data['word_count'], bins=20)\n",
    "plt.title('Word Count Distribution in Reviews')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cb48c9",
   "metadata": {},
   "source": [
    "### Observation - Review with a word count between 100 and 500 featured an average of about 8000 times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ada791",
   "metadata": {},
   "source": [
    "3. How does the word count of movie reviews relate to their sentiment? Is there a noticeable difference in word count between positive and negative reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21961f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box plot to visualize the relationship between sentiment and word count\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='sentiment', y='word_count', data=data)\n",
    "plt.title('Relationship between Sentiment and Word Count')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Word Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becf3520",
   "metadata": {},
   "source": [
    "### Observation - The average word count of words within the Positive class was about 500 while the average count of words within the Negative class averaged about 400 words. We can infer that longer reviews are likely have Positive Sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b00949c",
   "metadata": {},
   "source": [
    "## Feature Processing & Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3584738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hashtags from the 'review' column\n",
    "data['review'] = data['review'].str.replace(r'#\\w+', '', regex=True)\n",
    "\n",
    "#display dataframe\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy\n",
    "nlp = English()\n",
    "stopwords = list(nlp.Defaults.stop_words)\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7504cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text \n",
    "data['review'] = data['review'].apply(lambda text: text.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization, stopwords removal, and punctuation removal using spaCy\n",
    "def process_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if token.text not in stopwords and token.text not in punctuations]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890c84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('word_count', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962f42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91236b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the 'sentiment' column in your DataFrame\n",
    "data['sentiment'] = label_encoder.fit_transform(data['sentiment'])\n",
    "\n",
    "data['sentiment'].unique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16e4a13",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8588b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = data['review']\n",
    "y = data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66cf5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d59d729",
   "metadata": {},
   "source": [
    "## Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a19b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(process_text)\n",
    "X_test = X_test.apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text vectorization using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf134ed",
   "metadata": {},
   "source": [
    "## TRAIN & EVALUATE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38181af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Classifier', 'Accuracy'])\n",
    "\n",
    "from sklearn.svm import SVC  \n",
    "\n",
    "# List of classifiers\n",
    "classifiers = {\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"RandomForest\": RandomForestClassifier(),\n",
    "    \"SVC\": SVC()  \n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for clf_name, classifier in classifiers.items():\n",
    "    classifier.fit(X_train_tfidf, y_train)\n",
    "    y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "    # Calculate and print the accuracy score in percentage format\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "    print(f\"Accuracy Score for {clf_name}: {accuracy:.0f}%\\n\")\n",
    "    \n",
    "\n",
    "    # Append the results to the DataFrame\n",
    "    results_df = results_df.append({'Classifier': clf_name, 'Accuracy': accuracy}, ignore_index=True)\n",
    "\n",
    "    # Print the classification report\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(f\"Classification Report for {clf_name}:\\n{class_report}\\n\")\n",
    "\n",
    "results_df = results_df.sort_values(by='Accuracy', ascending=False)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe849fb",
   "metadata": {},
   "source": [
    "## CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f084d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each classifier in the dictionary\n",
    "for clf_name, classifier in classifiers.items():\n",
    "    # Fit the classifier on the training data\n",
    "    classifier.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = classifier.predict(X_test_tfidf)\n",
    "    \n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Print the confusion matrix distribution\n",
    "    print(f\"Confusion Matrix for {clf_name} Distribution:\")\n",
    "    print(pd.DataFrame(cm, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']))\n",
    "\n",
    "    # Plot the confusion matrix as a heatmap\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "    plt.title(f'Confusion Matrix for {clf_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce4ed3e",
   "metadata": {},
   "source": [
    "## HYPERPARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef39517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  \n",
    "    'penalty': ['l1', 'l2'] \n",
    "}\n",
    "\n",
    "# Create the logistic regression model with random state\n",
    "logistic_classifier = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(logistic_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "best_classifier = LogisticRegression(solver='liblinear', max_iter=1000, **best_params)\n",
    "\n",
    "best_classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c645e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set using the tuned model\n",
    "y_pred = best_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate and print the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy Score after Hyperparameter Tuning: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d5cae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Convert 'negative' to 0 and 'positive' to 1 in y_test\n",
    "y_test_binary = y_test.map({'negative': 0, 'positive': 1})\n",
    "\n",
    "# Get the predicted probabilities for the positive class\n",
    "y_prob = best_classifier.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "# Compute the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test_binary, y_prob)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4403b122",
   "metadata": {},
   "source": [
    "#### Observation - AUC ≥ 0.9: The model has outstanding discriminative ability. An ROC curve with an AUC of 0.94 means that the model is quite effective in classifying data points correctly, and it is considered a strong performer in this binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412a8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the components and model\n",
    "components_dict = {\n",
    "    'cleaner': process_text,\n",
    "    'cleaner_tfidf_vectorizer': tfidf_vectorizer,\n",
    "    'cleaner_classifier': best_classifier\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee0417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder to store the exported data\n",
    "folder_name = 'comp_folder'\n",
    "os.makedirs(folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fff7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the components and model\n",
    "components_path = os.path.join(folder_name, 'sentiment_components.pkl')\n",
    "with open(components_path, 'wb') as file:\n",
    "    pickle.dump(components_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a765a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the requirements.txt file\n",
    "requirements_file = 'requirements.txt'\n",
    "os.system(f'pip freeze > {os.path.join(folder_name, requirements_file)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0615c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the exported data folder\n",
    "shutil.make_archive('exported_data', 'zip', 'exported_comp_folder')\n",
    "\n",
    "print(\"Exported data has been zipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba64acb",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a21fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the exported components from the file\n",
    "components_path = 'comp_folder/sentiment_components.pkl'\n",
    "with open(components_path, 'rb') as file:\n",
    "    components_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380f738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the loaded components\n",
    "loaded_cleaner = components_dict['cleaner']\n",
    "loaded_vectorizer = components_dict['cleaner_tfidf_vectorizer']\n",
    "loaded_classifier = components_dict['cleaner_classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cb6095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new data from a CSV file\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256cd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.rename(columns={'content': 'review'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176330b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = data1['review_file'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86daf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.drop('review_file', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['review'] = data1['review'].apply(lambda text: text.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aa00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text cleaning to the new data\n",
    "data1['review'] = data1['review'].apply(loaded_cleaner)\n",
    "\n",
    "# Vectorize the new data using the loaded vectorizer\n",
    "X_new_tfidf = loaded_vectorizer.transform(data1['review'])\n",
    "\n",
    "# Make predictions on the new data\n",
    "new_predictions = loaded_classifier.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the \"review_file\" column back to the data1 DataFrame\n",
    "data1['review_file'] = files\n",
    "\n",
    "# Add the predictions to the new_data DataFrame\n",
    "data1['predicted_sentiment'] = new_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e89a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with only \"review_file\" and \"predicted_sentiment\" columns\n",
    "prediction = data1[['review_file', 'predicted_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54831dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051aed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of \"Positive\" and \"Negative\" in the predictions\n",
    "count = prediction['predicted_sentiment'].value_counts()\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38249d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Predictions\n",
    "prediction.to_csv('predicted_sentiment_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faae0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabdd60f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
